---
title: "Prior Weighted Mortality Analysis"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float:
      collapsed: false
    toc_depth: 3
---

```{r setGlobalChunkOptions, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(include = FALSE)
```

```{css echo = FALSE, include = TRUE}
/* Make top level headers bold in TOC*/
#TOC > ul > li {
  font-weight: bold;
}

/* Although tables are aligned to the left page border, offset this to the right. */
table {
  margin-left: 30px;
}

/* Put a gap between top-level bullet points and the prior element. */
div > ul > li {
  margin-top: 5px
}

/* Increase the vertical gap above L2 header. */
h2 {
  margin-top: 50px;
  text-decoration: underline;
}

/* Increase the vertical gap above L3 header. */
h3 {
  margin-top: 25px;
}
```

```{r loadLibraries, include=FALSE}
library (ggplot2, warn.conflicts = FALSE, quietly = TRUE)
library (GGally, warn.conflicts = FALSE, quietly = TRUE)
library (magrittr, warn.conflicts = FALSE, quietly = TRUE)
library (rstan, warn.conflicts = FALSE, quietly = TRUE)
```

```{r loadCurrentBasis}
totalRecords <- 100000L
stanChains <- 4L
stanIters <- 20000L

# priorParams <-
#   list(
#     `BASE|BASE` = c(MEAN = 0.0, SD = 0.01),
#     `GENDER|FEMALE` = c(MEAN = 0.0, SD = 0.1),
#     `HEALTH|BAD` = c(MEAN = 0.0, SD = 0.15),
#     `OCC_RISK|LOW` = c(MEAN = 0.0, SD = 0.01),
#     `OCC_RISK|HIGH` = c(MEAN = 0.0, SD = 0.25)
#   )

priorParams <-
  list(
    `BASE|BASE` = c(MEAN = 0.0, SD = 0.01),
    `GENDER|FEMALE` = c(MEAN = 0.0, SD = 0.01),
    `HEALTH|BAD` = c(MEAN = 0.0, SD = 0.01),
    `OCC_RISK|LOW` = c(MEAN = 0.0, SD = 0.01),
    `OCC_RISK|HIGH` = c(MEAN = 0.0, SD = 0.01)
  )

# Load in our expected basis.
currentBasis <-
  readxl::read_xlsx(
    path = 'PARAMETERS.XLSX',
    sheet = 'PARAMS',
    range = 'O1:Q9',
    col_types = c('text', 'text', 'numeric')
  ) %>%
  dplyr::mutate(
    LABEL =
      glue::glue('{DIMENSION}|{LEVEL}') %>%
      # Convert to an ordered factor so that this specific ordering is maintained on visual outputs.
      forcats::fct_inorder(ordered = TRUE)
  )
```

```{r generatePolicyData}
polData <-
  purrr::reduce(
    c('BASE', 'GENDER', 'HEALTH', 'OCC_RISK'),
    function (prior, dimension) {
      prior %>%
        dplyr::left_join(
          currentBasis %>%
            dplyr::filter(
              DIMENSION == dimension
            ) %>%
            dplyr::transmute(
              '{dimension}' := LEVEL,
              '{dimension}_LOADING' := RATE
            ),
          by = dimension
        )
    },
    .init =
      readxl::read_xlsx(
        path = 'PARAMETERS.XLSX',
        sheet = 'PARAMS',
        range = 'A1:E13',
        col_types = c('text', 'text', 'text', 'numeric', 'numeric')
      ) %>%
      dplyr::mutate(
        BASE = 'BASE'
      )
  ) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    EXPECTED_RATE =
      prod(
        dplyr::c_across(
          dplyr::ends_with('LOADING')
        )
      )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::select(
    -BASE,
    -dplyr::ends_with('LOADING')
  ) %>%
  dplyr::mutate(
    # Ordered so as particular levels used for GLM reference levels.
    GENDER =
      factor(GENDER, levels = c('MALE', 'FEMALE')),
    HEALTH =
      factor(HEALTH, levels = c('GOOD', 'BAD')),
    OCC_RISK =
      factor(OCC_RISK, levels = c('MEDIUM', 'LOW', 'HIGH')),
    # Ordered another way for purpose of visuals.
    GENDER_ALT = GENDER,
    HEALTH_ALT = HEALTH,
    OCC_RISK_ALT =
      OCC_RISK %>%
      factor(levels = c('LOW', 'MEDIUM', 'HIGH')),
    EXPOSURE =
      as.integer(totalRecords * PROPORTION),
    ACTUAL_DEATHS =
      as.integer(EXPOSURE * ACTUAL_RATE),
    EXPECTED_DEATHS =
      EXPOSURE * EXPECTED_RATE
  ) %>%
  dplyr::arrange(
    GENDER_ALT,
    HEALTH_ALT,
    OCC_RISK_ALT
  )
```


## Purpose

The purpose of this note is to show how **Bayesian regression** can be used as an alternative to a GLM approach when carrying out an experience analysis investigation; furthermore, it can provide a fit of parameter estimates that also allow for any _prior_ belief in what those estimates should be.

Using a fabricated data-set, the analysis is firstly performed using a GLM approach, before repeating the exercise in a Bayesian setting; the former being done (if nothing else) to better highlight the benefit of the latter.


## Background

For some products/basis items, proposed basis changes are made by (at least) considering multiple one-way analyses of actual vs expected experience. Prior to any proposed changes, a "mechanical-rule" process is applied to determine whether any such departure to expected experience is sufficient enough to warrant a change in basis. (**Correct?**).

However, one shortcoming with this approach (in the author's opinion) is that this rule is only being applied when considering individual one-way analyses of results. For example, deciding to not change the gender loading could mean that another assumption could well need to change to compensate for any such (no) change.

Ideally, any such adjustment should be done considering all basis dimensions simultaneously. Through the use of Bayesian regression (as opposed to a "vanilla" GLM regression), we can obtain weighted estimates that:

* Considers **all** basis dimensions (eg... gender, occupation, ...) simultaneously.
* Reflects the amount of exposure underlying corresponding cells.
* Reflects our **prior beliefs** about what those estimates should be.

Additionally, the Bayesian framework provides a joint probability distribution for our parameter estimates with which we can carry out any analysis/inference as needed.


## Setup

The current assumed mortality basis is structured as follows:

* A base mortality rate of $\mu_{BASE} =`r currentBasis %>% dplyr::filter(DIMENSION == 'BASE') %>% dplyr::pull(RATE) %>% scales::percent(accuracy = 0.1, suffix = '\\%')`$; this is assumed to apply to **males** of **good health** and **medium occupation risk**.
* Proportional loadings are subsequently applied depending on gender ($G$), health ($H$) and occupational risk ($O$); these are shown in the tables below.
* The deaths over a given unit-exposure period for a life belonging to the $i^{th}$ exposure cell are assumed to be Poisson distributed with rate parameter ($\lambda$) equal to the base mortality rate, cumulatively multiplied by the corresponding loadings; let the overall mortality rate for this $i^{th}$ cell be $\mu_i = \mu_{BASE} \cdot G_i \cdot H_i \cdot O_i$, where:

$$
\begin{align}
  G_i &=
    \begin{cases}
      1, & \text{if }i^{th}\text{ cell is for males.} \\
      G_{FEMALE}, & \text{if }i^{th}\text{ cell is for females.}
    \end{cases} \\[2.0ex]
  H_i &=
    \begin{cases}
      1, & \text{if }i^{th}\text{ cell is for good health lives.} \\
      H_{BAD}, & \text{if }i^{th}\text{ cell is for bad health lives.}
    \end{cases} \\[2.0ex]
  O_i &=
    \begin{cases}
      O_{LOW}, & \text{if }i^{th}\text{ cell is for low-risk occupations.} \\
      1, & \text{if }i^{th}\text{ cell is for medium-risk occupations.} \\
      O_{HIGH}, & \text{if }i^{th}\text{ cell is for high-risk occupations.}
    \end{cases}
\end{align}
$$

* Considering the **total** exposure ($E_i$) and deaths ($D_i$) for a given cell, assuming that underlying lives are independent, the deaths over a given cell can therefore be assumed to be distributed according to $D_i \sim \operatorname{Poisson}(\lambda=E_i\cdot\mu_i)$.
* Values shown in <span style="background-color: yellow;">yellow</span> are taken to be **reference levels** and are already allowed for within the base rate above.

```{r showMortalityLoadings, warning=FALSE, include=TRUE, results='asis'}
shiny::tags$table(
  shiny::tags$tr(
    currentBasis %>%
      dplyr::transmute(
        DIMENSION,
        LEVEL,
        LOADING =
          scales::percent(RATE, accuracy = 0.1)
      ) %>%
      dplyr::filter(
        DIMENSION != 'BASE'
      ) %>%
      dplyr::group_by(DIMENSION) %>%
      dplyr::group_map(
        ~ knitr::kable(
            .x %>%
              dplyr::rename('{.y}' := LEVEL),
            format = 'html',
            align = 'rc'
          ) %>%
          kableExtra::kable_styling(
            bootstrap_options = 'bordered',
            full_width = FALSE
          ) %>%
          kableExtra::row_spec(
            row = which(.x$LOADING == '100.0%'),
            background = 'yellow'
          ) %>%
          kableExtra::column_spec(
            column = 1L,
            background = '#EEEEEE'
          ) %>%
          kableExtra::row_spec(
            row = 0L,
            bold = TRUE,
            background = '#BBBBBB'
          ) %>%
          as.character() %>%
          # Treat as already escaped HTML.
          htmltools::HTML() %>%
          # Set gap between each of the tables to be shown.
          shiny::tags$td(
            style = 'padding-right: 25px; vertical-align: top'
          )
      )
  )
)
```

<br/>

A number of 'actual' deaths have been recorded along with corresponding exposure years. Below shows these split by all cell combinations, the implied actual rate and how this compares to the expected rated derived from the loadings above:

```{r showActuals, include = TRUE}
local({
  preTotals <-
    polData %>%
    dplyr::arrange(
      GENDER_ALT,
      HEALTH_ALT,
      OCC_RISK_ALT
    ) %>%
    dplyr::transmute(
      GENDER,
      HEALTH,
      OCC_RISK,
      EXPOSURE,
      ACTUAL_DEATHS,
      EXPECTED_DEATHS,
      ACTUAL_RATE,
      EXPECTED_RATE
    )
  
  totalRow <-
    polData %>%
    dplyr::summarise(
      OCC_RISK = 'TOTAL',
      EXPECTED_RATE = sum(EXPOSURE * EXPECTED_RATE) / sum(EXPOSURE),
      EXPOSURE = sum(EXPOSURE),
      ACTUAL_DEATHS = sum(ACTUAL_DEATHS),
      EXPECTED_DEATHS = sum(EXPECTED_DEATHS),
      ACTUAL_RATE = ACTUAL_DEATHS / EXPOSURE
    )
  
  preTotals %>%
    dplyr::bind_rows(totalRow) %>%
    dplyr::transmute(
      GENDER =
        GENDER %>%
        as.character() %>%
        tidyr::replace_na(replace = ''),
      HEALTH =
        HEALTH %>%
        as.character() %>%
        tidyr::replace_na(replace = ''),
      OCC_RISK,
      EXPOSURE =
        scales::comma(EXPOSURE),
      EXPECTED_DEATHS =
        scales::comma(EXPECTED_DEATHS, accuracy = 1.0),
      ACTUAL_DEATHS =
        scales::comma(ACTUAL_DEATHS, accuracy = 1.0),
      EXPECTED_RATE =
        scales::percent(EXPECTED_RATE, accuracy = 0.1),
      ACTUAL_RATE =
        scales::percent(ACTUAL_RATE, accuracy = 0.1)
    ) %>%
    knitr::kable(
      format = 'html',
      align = 'ccccccc'
    ) %>%
    kableExtra::kable_styling(
      bootstrap_options = 'bordered',
      full_width = FALSE
    ) %>%
    kableExtra::column_spec(
      column = 1L:3L,
      background = '#EEEEEE'
    ) %>%
    kableExtra::row_spec(
      row = 0L,
      bold = TRUE,
      background = '#BBBBBB'
    ) %>%
    kableExtra::row_spec(
      row = nrow(preTotals) + 1L,
      bold = TRUE,
      background = '#EEEEEE'
    )
})
```


## Proposal (Poisson GLM)

With a suitable GLM model specified, we can determine "best-fit" adjustments by which our expected loadings and base rate above should change in order to best replicate the actual experience.

Specifically, we would like to find $\mu_{BASE}^{ADJ}$, $G_i^{ADJ}$, $H_i^{ADJ}$ and $O_i^{ADJ}$ that maximizes the likelihood of:

$$D_i \sim \operatorname{Poisson}(\lambda = E_i \cdot \mu_i \cdot \mu_{BASE}^{ADJ} \cdot G_i^{ADJ} \cdot H_i^{ADJ} \cdot O_i^{ADJ})$$
The inclusion of $E_i$ and $\mu_i$ means that our parameters estimates will be relative to our current expected basis.

Where:
$$
\begin{align}
  G_i^{ADJ} &=
    \begin{cases}
      1, & \text{if }i^{th}\text{ cell is for males.} \\
      G_{FEMALE}^{ADJ}, & \text{if }i^{th}\text{ cell is for females.}
    \end{cases} \\[2.0ex]
  H_i^{ADJ} &=
    \begin{cases}
      1, & \text{if }i^{th}\text{ cell is for good health lives.} \\
      H_{BAD}^{ADJ}, & \text{if }i^{th}\text{ cell is for bad health lives.}
    \end{cases} \\[2.0ex]
  O_i^{ADJ} &=
    \begin{cases}
      O_{LOW}^{ADJ}, & \text{if }i^{th}\text{ cell is for low-risk occupations.} \\
      1, & \text{if }i^{th}\text{ cell is for medium-risk occupations.} \\
      O_{HIGH}^{ADJ}, & \text{if }i^{th}\text{ cell is for high-risk occupations.}
    \end{cases}
\end{align}
$$

<br/>

Noting that where males have been chosen as a reference level, an adjustment will only be determined for $G_{FEMALE}$. Similarly, adjustments will only be derived for $H_{BAD}$, $O_{LOW}$ and $O_{HIGH}$ as good health and medium occupation risk are the reference levels respectively.

However, the GLM framework requires that we fit a **linear combination** of our parameters meaning that the problem, as currently framed, is not possible. We can instead define:

$$
\begin{align}
  \mu_{BASE}^{ADJ*} &= \operatorname{log}(\mu_{BASE}^{ADJ}) \\
  G_{FEMALE}^{ADJ*} &= \operatorname{log}(G_{FEMALE}^{ADJ}) \\
  H_{BAD}^{ADJ*} &= \operatorname{log}(H_{BAD}^{ADJ}) \\
  O_{LOW}^{ADJ*} &= \operatorname{log}(O_{LOW}^{ADJ}) \\
  O_{HIGH}^{ADJ*} &= \operatorname{log}(O_{HIGH}^{ADJ})
\end{align}
$$

From these, we define $G_i^{ADJ*}$, $H_i^{ADJ*}$ and $O_i^{ADJ*}$ as expected.

Noting that a similar transformation has also been applied to $E_i \cdot \mu_i$, our problem can then be rewritten as maximising the probability of:

$$D_i \sim \operatorname{Poisson}(\lambda = e^{\operatorname{log}(E_i \cdot \mu_i)} \cdot e^{\mu_{BASE}^{ADJ*}} \cdot e^{G_i^{ADJ*}} \cdot e^{H_i^{ADJ*}} \cdot e^{O_i^{ADJ*}})$$
Which is equivalent to:

$$D_i \sim \operatorname{Poisson}(\lambda = \operatorname{exp}\{\operatorname{log}(E_i) + \operatorname{log}(\mu_i) + \mu_{BASE}^{ADJ*} + G_i^{ADJ*} + H_i^{ADJ*} + O_i^{ADJ*}\})$$
We now have an expression for $\lambda$ that has been written as the exponential transformation of a linear combination of parameters.

Fitting a Poisson distribution is readily possible within the GLM framework, however the exponential transformation above requires that we use a "log-link" function; however, this is more of an implementation concern. It is worth noting though that this transformation ensures that the fitted value for $\lambda$ will always be non-negative, as required by the Poisson distribution.

### GLM Output

The R code uses the standard `glm` function as shown below; note how the structure to the right of `~` is as per the structure proposed above:

```{r runGLMFit, echo = TRUE, include = TRUE, results = 'hide'}
glmFit <-
  glm(
    ACTUAL_DEATHS ~ offset(log(EXPOSURE) + log(EXPECTED_RATE)) + GENDER + HEALTH + OCC_RISK,
    data = polData,
    family = poisson(link = 'log')
  )
```

Executing this, provides us with the following estimates of $\mu_{BASE}^{ADJ*}$, $G_i^{ADJ*}$, ... and hence $\mu_{BASE}^{ADJ}$, $G_i^{ADJ}$, ... where (for example) $\mu_{BASE}^{ADJ} = \operatorname{exp}(\mu_{BASE}^{ADJ*})$.

Note that:

* Adjustments of 100% have been manually inserted for our reference levels.
* The **proposed rates** are equal to current (ie. expected) rates multiplied by the GLM fitted adjustment.

```{r getGLMEstimates}
glmEstimates <-
  currentBasis %>%
  dplyr::mutate(
    term =
      glue::glue('{DIMENSION}{LEVEL}')
  ) %>%
  dplyr::select(-RATE) %>%
  dplyr::left_join(
    glmFit %>%
      broom::tidy(exponentiate = TRUE) %>%
      dplyr::mutate(
        term =
          dplyr::if_else(term == '(Intercept)', 'BASEBASE', term)
      ),
    by = 'term'
  ) %>%
  dplyr::transmute(
    DIMENSION =
      tidyr::replace_na(DIMENSION, replace = 'BASE'),
    LEVEL =
      tidyr::replace_na(LEVEL, replace = 'BASE'),
    FITTED_ADJUSTMENT =
      estimate %>%
      tidyr::replace_na(replace = 1.0)
  ) %>%
  dplyr::left_join(
    currentBasis %>%
      dplyr::rename(
        CURRENT = RATE
      ),
    by = c('DIMENSION', 'LEVEL')
  ) %>%
  dplyr::mutate(
    GLM_PROPOSED =
      CURRENT * FITTED_ADJUSTMENT
  )
```

```{r showGlMEstimates, include = TRUE}
glmEstimates %>%
  dplyr::select(
    -LABEL
  ) %>%
  dplyr::mutate_if(
    is.numeric,
    scales::percent_format(accuracy = 0.1)
  ) %>%
  knitr::kable(
    format = 'html',
    align = 'ccc'
  ) %>%
  kableExtra::kable_styling(
    bootstrap_options = 'bordered',
    full_width = FALSE
  ) %>%
  kableExtra::column_spec(
    column = 1L:2L,
    background = '#EEEEEE'
  ) %>%
  kableExtra::row_spec(
    row = 0L,
    bold = TRUE,
    background = '#BBBBBB'
  )
```

<br/>

```{r augmentPolicyDataGLM}
augmentedPolDataGLM <-
  purrr::reduce(
    c('BASE', 'GENDER', 'HEALTH', 'OCC_RISK'),
    function (prior, dimension) {
      prior %>%
        # Without this, we seem to lose the ordered factors within
        # the respective dimensional columns.
        dplyr::mutate(
          LEVEL = .data[[dimension]]
        ) %>%
        dplyr::left_join(
          glmEstimates %>%
            dplyr::filter(
              DIMENSION == dimension
            ) %>%
            dplyr::transmute(
              LEVEL,
              '{dimension}_LOADING' := GLM_PROPOSED
            ),
          by = 'LEVEL'
        ) %>%
        dplyr::select(
          -LEVEL
        )
    },
    .init =
      polData %>%
      dplyr::mutate(
        BASE = 'BASE'
      )
  ) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    GLM_RATE =
      prod(
        dplyr::c_across(
          dplyr::ends_with('LOADING')
        )
      )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(
    GENDER_ALT,
    HEALTH_ALT,
    OCC_RISK_ALT
  ) %>%
  dplyr::transmute(
    GENDER,
    HEALTH,
    OCC_RISK,
    GENDER_ALT,
    HEALTH_ALT,
    OCC_RISK_ALT,
    PROPORTION,
    EXPOSURE,
    ACTUAL_RATE,
    EXPECTED_RATE,
    GLM_RATE,
    ACTUAL_DEATHS,
    EXPECTED_DEATHS,
    GLM_DEATHS =
      EXPOSURE * GLM_RATE
  )
```

:::: {}

### Proposed Fit {.tabset .tabset-fade .tabset-pills}

Allowing for the newly proposed rates above, we have a perfect fit between actual and proposed deaths at the **aggregate level**; however, less so at the individual cell level.

```{r prepareGLMFitTable}
glmFitTable <-
  local({
    preTotals <-
      augmentedPolDataGLM %>%
      dplyr::transmute(
        GENDER,
        HEALTH,
        OCC_RISK,
        EXPOSURE,
        ACTUAL_DEATHS,
        EXPECTED_DEATHS,
        GLM_DEATHS,
        ACTUAL_RATE,
        EXPECTED_RATE,
        GLM_RATE
      )
    
    totalRow <-
      augmentedPolDataGLM %>%
      dplyr::summarise(
        OCC_RISK = 'TOTAL',
        EXPECTED_RATE = sum(EXPOSURE * EXPECTED_RATE) / sum(EXPOSURE),
        GLM_RATE = sum(EXPOSURE * GLM_RATE) / sum(EXPOSURE),
        EXPOSURE = sum(EXPOSURE),
        ACTUAL_DEATHS = sum(ACTUAL_DEATHS),
        EXPECTED_DEATHS = sum(EXPECTED_DEATHS),
        GLM_DEATHS = sum(GLM_DEATHS),
        ACTUAL_RATE = ACTUAL_DEATHS / EXPOSURE
      )
    
    preTotals %>%
      dplyr::bind_rows(totalRow) %>%
      dplyr::transmute(
        GENDER =
          GENDER %>%
          as.character() %>%
          tidyr::replace_na(replace = ''),
        HEALTH =
          HEALTH %>%
          as.character() %>%
          tidyr::replace_na(replace = ''),
        OCC_RISK,
        EXPOSURE =
          scales::comma(EXPOSURE),
        EXPECTED_DEATHS =
          scales::comma(EXPECTED_DEATHS, accuracy = 1.0),
        ACTUAL_DEATHS =
          scales::comma(ACTUAL_DEATHS, accuracy = 1.0),
        GLM_DEATHS =
          scales::comma(GLM_DEATHS, accuracy = 1.0),
        EXPECTED_RATE =
          scales::percent(EXPECTED_RATE, accuracy = 0.1),
        ACTUAL_RATE =
          scales::percent(ACTUAL_RATE, accuracy = 0.1),
        GLM_RATE =
          scales::percent(GLM_RATE, accuracy = 0.1)
      )
  })

glmFitFormat <-
  function (data) {
    data %>%
      knitr::kable(
        format = 'html',
        align = 'ccccccc'
      ) %>%
      kableExtra::kable_styling(
        bootstrap_options = 'bordered',
        full_width = FALSE
      ) %>%
      kableExtra::column_spec(
        column = 1L:3L,
        background = '#EEEEEE'
      ) %>%
      kableExtra::row_spec(
        row = 0L,
        bold = TRUE,
        background = '#BBBBBB'
      ) %>%
      kableExtra::row_spec(
        row = nrow(data),
        bold = TRUE,
        background = '#EEEEEE'
      )
  }
```

#### Deaths

```{r showGLMDeaths, include = TRUE}
glmFitTable %>%
  dplyr::select(
    -dplyr::ends_with('RATE')
  ) %>%
  dplyr::rename_with(
    stringr::str_remove,
    pattern = '_DEATHS$'
  ) %>%
  glmFitFormat()
```

#### Rates

```{r showGLMRates, include = TRUE}
glmFitTable %>%
  dplyr::select(
    -dplyr::ends_with('DEATHS')
  ) %>%
  dplyr::rename_with(
    stringr::str_remove,
    pattern = '_RATE$'
  ) %>%
  glmFitFormat()
```

::::

We can visualise this in a waterfall graph as shown below:

* Within each cell grouping, the top bar represents the difference between **actual** and **expected** deaths.
* The lower bar represents the difference between **actual** deaths and deaths as would be expected under the newly **proposed GLM** basis; <span style="color: green;">green</span>/<span style="color: red;">red</span> indicate a closer/further fit respectively between the proposed and actual deaths, relative to those originally expected.
* As seen, some cells subsequently achieve a closer fit but not all.

```{r showWaterfallGraphGLM, include = TRUE}
local({
  plotData <-
    augmentedPolDataGLM %>%
    dplyr::mutate(
      IDX =
        rev(dplyr::row_number()),
      CELL_LABEL =
        glue::glue('{GENDER}|{HEALTH}|{OCC_RISK}'),
      EXPECTED_HJUST =
        dplyr::if_else(
          EXPECTED_DEATHS < ACTUAL_DEATHS, 1.0, 0.0
        ),
      ACTUAL_HJUST =
        1.0 - EXPECTED_HJUST,
      GLM_HJUST =
        dplyr::if_else(
          GLM_DEATHS < ACTUAL_DEATHS, 1.0, 0.0
        ),
      GLM_IS_BETTER =
        (GLM_DEATHS - ACTUAL_DEATHS)^2 < (EXPECTED_DEATHS - ACTUAL_DEATHS)^2
    )
  
  ggplot(data = plotData) +
    geom_rect(
      aes(
        xmin = IDX,
        xmax = IDX + 0.5,
        ymin = EXPECTED_DEATHS,
        ymax = ACTUAL_DEATHS
      ),
      fill = '#AAAAFF',
    ) +
    geom_segment(
      aes(
        x= IDX + 0.25,
        xend = IDX + 0.25,
        y = EXPECTED_DEATHS,
        yend = ACTUAL_DEATHS
      ),
      arrow =
        ggplot2::arrow(
          length = unit(0.15, units = 'cm'),
          type = 'closed'
        )
    ) +
    geom_rect(
      aes(
        xmin = IDX - 0.5,
        xmax = IDX,
        ymin = ACTUAL_DEATHS,
        ymax = GLM_DEATHS,
        fill = GLM_IS_BETTER
      )
    ) +
    geom_segment(
      aes(
        x= IDX - 0.25,
        xend = IDX - 0.25,
        y = ACTUAL_DEATHS,
        yend = GLM_DEATHS,
        colour = GLM_IS_BETTER
      ),
      arrow =
        ggplot2::arrow(
          length = unit(0.15, units = 'cm'),
          type = 'closed'
        )
    ) +
    geom_text(
      aes(
        x = IDX + 0.25,
        y = EXPECTED_DEATHS,
        label = substr(SOURCE, 1L, 1L),
        hjust = EXPECTED_HJUST
      ),
      label = 'E',
      vjust = 0.5
    ) +
    geom_text(
      aes(
        x = IDX + 0.25,
        y = ACTUAL_DEATHS,
        hjust = ACTUAL_HJUST
      ),
      label = 'A',
      vjust = 0.5
    ) +
    geom_text(
      aes(
        x = IDX - 0.25,
        y = GLM_DEATHS,
        hjust = GLM_HJUST
      ),
      label = 'P',
      vjust = 0.5
    ) +
    scale_x_continuous(
      name = 'Cell',
      breaks = plotData$IDX,
      labels = plotData$CELL_LABEL
    ) +
    scale_y_continuous(
      name = 'Deaths',
      labels = scales::label_comma(),
    ) +
    scale_fill_manual(
      values = c(`TRUE` = '#AAFFAA', `FALSE` = '#FFAAAA')
    ) +
    scale_colour_manual(
      values = c(`TRUE` = 'darkgreen', `FALSE` = 'darkred')
    ) +
    coord_flip() +
    ggtitle(
      label = 'Walk from Expected 🡆 Actual 🡆 GLM Proposed'
    ) +
    cowplot::theme_half_open() +
    cowplot::background_grid(major = 'xy', minor = 'none') +
    theme(
      plot.title = element_text(size = 10.0),
      legend.position = 'none'
    )
})
```


### Summary

Although a perfect fit has been obtained at the **aggregate** level, the fitting process has ignored any prior beliefs we might have in our current expected basis.

It would be preferable if the fitting process could explicitly consider our preference for the current basis, quantified in some manner, departing only where the extent of movement and/or quantity of data demands it.

Such an approach is possible with Bayesian regression as described below.


## Proposal (Bayesian)

The parameter estimates underlying the GLM framework above are derived via a maximum-likelihood approach; specifically, the parameter estimates are determined such that the probability of getting the actual observations, conditional on those parameter estimates, is maximised:

$$\underset{\mu_{BASE}^{ADJ},G_{FEMALE}^{ADJ},...,O_{HIGH}^{ADJ}}{\operatorname{arg max}}\operatorname{P}(D_i|E_i,\mu_{BASE}^{ADJ},G_{FEMALE}^{ADJ},...,O_{HIGH}^{ADJ})$$

Under the Bayesian approach, we "flip" this around and instead look at the **posterior distribution** of those parameters given the data, rather than point-estimates for those parameters. That is, the probability of those parameter estimates given the data. Via Bayes theorem, it can be shown that:

$$\underset{\texttt{POSTERIOR}}{\operatorname{P}(\mu_{BASE}^{ADJ},...,O_{HIGH}^{ADJ}|D_i,E_i)} \propto \underset{\texttt{LIKELIHOOD}}{\operatorname{P}(D_i|E_i,\mu_{BASE}^{ADJ},...,O_{HIGH}^{ADJ})} \cdot \underset{\texttt{PRIOR}}{\operatorname{P}(\mu_{BASE}^{ADJ}...,O_{HIGH}^{ADJ})}$$
Specifically, that our **posterior** parameter distribution is proportional to our **likelihood** (as underlying the GLM approach) multiplied by the **prior** parameter distribution. Underlying the right-hand side of the above, there is a normalisation constant which can be ignored for purpose of this discussion.

Assuming that the prior parameter distributions are pairwise independent, this "simplifies" to:

$$\underset{\texttt{POSTERIOR}}{\operatorname{P}(\mu_{BASE}^{ADJ},...,O_{HIGH}^{ADJ}|D_i,E_i)} \propto \underset{\texttt{LIKELIHOOD}}{\operatorname{P}(D_i|E_i,\mu_{BASE}^{ADJ},...,O_{HIGH}^{ADJ})} \cdot \color{red}{\underset{\texttt{PRIOR}}{\operatorname{P}(\mu_{BASE}^{ADJ}) \cdot ... \cdot \operatorname{P}(O_{HIGH}^{ADJ})}}$$

As such, this approach requires us to have a **prior** opinion regarding those parameters estimates, as represented in <span style="color: red;">red</span> above.

Whereas the **vanilla GLM** approach assumes that the parameters have fixed, albeit unknown values, the **Bayesian** approach instead assumes that they themselves are also random, but with a known distribution; this highlights one such difference between these two approaches.

### Determining Posterior Distribution

Assuming suitable priors can be found... **How can we derive the posterior distribution?**

* **Analytically**; only possible with certain combinations of likelihood and prior distributions, such as (for example) conjugate priors.
* **Grid-based search**; the joint value space for **all** parameters is split into a multi-dimensional grid where the priors and likelihood are evaluated at each point. However, the number of points to be visited increases dramatically with the number of parameters, each one of which represents a single dimension to be considered in this grid.
* **Random sampling**; generating samples as if they had been randomly sampled from the posterior distribution itself.

Given the inherent complexity here, the first approach is not assumed to be possible.

Under the second approach, with our 5x parameters and assuming a modest (say) 50x points to be evaluated in each dimension, we have $50^{5} = 3.1\times10^8$ points to consider.

Therefore, an alternative needs to be found that can search our highly-dimensional sample space in a more efficient manner. This is possible using a **MCMC sampler** (Markov Chain Monte Carlo) which achieves the final approach referred to above.


### Prior Assumptions

Irrespective of the methods above, we require a set of prior assumptions as to our basis adjustments.

During the discussion of the GLM approach, reference was made to a "log-link" function; this is still required here in order to ensure that the overall Poisson rate parameter ($\lambda$) remains non-negative. This required the introduction of $\mu_{BASE}^{ADJ*}$, ..., $O_i^{ADJ*}$ which will also be used here.

Any prior assumptions we make will be regarding those same $*$ versions referred to above.

We _might_ therefore assume "tight" variances, placing greater weight on the current expected basis; noting that although we do not _expect_ any movements (ie. all have zero mean), we do however accept that there is some underlying variation, the extent of which can (if desired) differ by parameter.

$$\begin{aligned}
  \operatorname{log}(\mu_{BASE}^{ADJ}) &= \mu_{BASE}^{ADJ*} &\sim \operatorname{Normal}(`r priorParams[['BASE|BASE']]['MEAN']`, `r priorParams[['BASE|BASE']]['SD']`) \\
  \operatorname{log}(G_{FEMALE}^{ADJ}) &= G_{FEMALE}^{ADJ*} &\sim \operatorname{Normal}(`r priorParams[['GENDER|FEMALE']]['MEAN']`, `r priorParams[['GENDER|FEMALE']]['SD']`) \\
  \operatorname{log}(H_{BAD}^{ADJ}) &= H_{BAD}^{ADJ*} &\sim \operatorname{Normal}(`r priorParams[['HEALTH|BAD']]['MEAN']`, `r priorParams[['HEALTH|BAD']]['SD']`) \\
  \operatorname{log}(O_{LOW}^{ADJ}) &= O_{LOW}^{ADJ*} &\sim \operatorname{Normal}(`r priorParams[['OCC_RISK|LOW']]['MEAN']`, `r priorParams[['OCC_RISK|LOW']]['SD']`) \\
  \operatorname{log}(O_{HIGH}^{ADJ}) &= O_{HIGH}^{ADJ*} &\sim \operatorname{Normal}(`r priorParams[['OCC_RISK|HIGH']]['MEAN']`, `r priorParams[['OCC_RISK|HIGH']]['SD']`)
\end{aligned}$$


### Generating Posterior Samples

```{r loadStanModel}
stanCode <-
  readr::read_file(
    file = 'GLM_FIT.STAN'
  )
```

```{r compileStanModel}
stanModel <-
  xfun::cache_rds({
    rstan::stan_model(
      model_name = 'BAYESIAN_GLM',
      model_code = stanCode,
      save_dso = TRUE,
      auto_write = TRUE
    )
  }, hash = list(stanCode), dir = 'cache/')
```

```{r getModelMatrix}
stanModelMatrix <-
  model.matrix(glmFit)
```

```{r runStanModel}
stanFit <-
  xfun::cache_rds({
    rstan::sampling(
      stanModel,
      pars =
        c('baseAdj',
          'genderFemaleAdj',
          'healthBadAdj',
          'occRiskLowAdj',
          'occRiskHighAdj'),
      include = TRUE,
      chains = stanChains,
      cores = 2L,
      iter = stanIters,
      data =
        list(
          N = nrow(stanModelMatrix),
          baseAdjMean = priorParams$`BASE|BASE`['MEAN'],
          baseAdjSD = priorParams$`BASE|BASE`['SD'],
          genderFemaleAdjMean = priorParams$`GENDER|FEMALE`['MEAN'],
          genderFemaleAdjSD = priorParams$`GENDER|FEMALE`['SD'],
          healthBadAdjMean = priorParams$`HEALTH|BAD`['MEAN'],
          healthBadAdjSD = priorParams$`HEALTH|BAD`['SD'],
          occRiskLowAdjMean = priorParams$`OCC_RISK|LOW`['MEAN'],
          occRiskLowAdjSD = priorParams$`OCC_RISK|LOW`['SD'],
          occRiskHighAdjMean = priorParams$`OCC_RISK|HIGH`['MEAN'],
          occRiskHighAdjSD = priorParams$`OCC_RISK|HIGH`['SD'],
          genderFemale = stanModelMatrix[,'GENDERFEMALE'],
          healthBad = stanModelMatrix[,'HEALTHBAD'],
          occRiskLow = stanModelMatrix[,'OCC_RISKLOW'],
          occRiskHigh = stanModelMatrix[,'OCC_RISKHIGH'],
          D_expected = polData$EXPECTED_DEATHS,
          D_actual = polData$ACTUAL_DEATHS
        )
    )
  }, hash = list(stanCode, priorParams, stanChains, stanIters, stanModelMatrix, polData), dir = 'cache/')
```

```{r extractStanSamples}
stanSamples <-
  rstan::extract(stanFit) %>%
  tibble::as_tibble() %>%
  dplyr::transmute(
    `BASE|BASE` = baseAdj,
    `GENDER|FEMALE` = genderFemaleAdj,
    `HEALTH|BAD` = healthBadAdj,
    `OCC_RISK|LOW` = occRiskLowAdj,
    `OCC_RISK|HIGH` = occRiskHighAdj
  )
```

```{r extractStanEstimates}
stanEstimates <-
  currentBasis %>%
  dplyr::mutate(
    # This is done so that we don't lose the factor type on the LABEL column after joining.
    LABEL_CHAR =
      as.character(LABEL)
  ) %>%
  dplyr::left_join(
    rstan::summary(stanFit) %>%
      magrittr::extract2('summary') %>%
      magrittr::extract(1L:5L,1L) %>%
      purrr::set_names(
        dplyr::case_match,
        'baseAdj' ~ 'BASE|BASE',
        'genderFemaleAdj' ~ 'GENDER|FEMALE',
        'healthBadAdj' ~ 'HEALTH|BAD',
        'occRiskLowAdj' ~ 'OCC_RISK|LOW',
        'occRiskHighAdj' ~ 'OCC_RISK|HIGH'
      ) %>%
      tibble::enframe(
        name = 'LABEL_CHAR',
        value = 'FITTED_ADJUSTMENT'
      ),
    by = 'LABEL_CHAR'
  ) %>%
  dplyr::transmute(
    LABEL,
    DIMENSION,
    LEVEL,
    FITTED_ADJUSTMENT =
      FITTED_ADJUSTMENT %>%
      tidyr::replace_na(replace = 1.0),
    CURRENT = RATE,
    BAYESIAN_PROPOSED =
      CURRENT * FITTED_ADJUSTMENT
  )
```

There is a popular MCMC framework called [Stan](https://mc-stan.org/) which allows the user to specify the structure of the model, both likelihood and priors. The model is then run in order to generate **posterior samples** for our parameters, $\mu_{BASE}^{ADJ}$, ..., $O_{HIGH}^{ADJ}$, conditional on the observed deaths, exposure and assumed prior parameter distributions.

The **Stan** code used can be found `r xfun::embed_file(path = 'GLM_FIT.STAN', text = 'here')`.

Running this for `r scales::comma(nrow(stanSamples))` iterations has provided us with the same number of samples from our multi-dimensional **posterior** distribution. We can visualise this by considering all pairwise combinations of parameters, as shown below; the visualisation provides all of:

* A density plot of the **marginal** posterior distributions for each parameter along the diagonal.
* A heat map of the joint posterior distribution for each pair of parameters in the lower-left portion; the dashed lines intersect at the corresponding estimates for each parameter.
  * This also shows (visually) the extent of any pairwise correlations and the level of uncertainty over our parameter values.
  * All other things equal, the uncertainty could be reduced if (for example), our prior variances were reduced, and/or an increased number of samples were run.
* Pairwise correlations in the upper-right portion.
  * Note the negative correlation of the base adjustment ($\mu_{BASE}^{ADJ}$) with all other parameters. Intuitively, this makes sense as any increase in the base adjustment would require a corresponding reduction in all other loadings which are cumulatively applied to it.

```{r showStanPairwisePlot, include = TRUE}
local({
  stanEstimatesVector <-
    stanEstimates %>%
    dplyr::transmute(
      LABEL = glue::glue('{DIMENSION}|{LEVEL}'),
      FITTED_ADJUSTMENT
    ) %>%
    tibble::deframe()
  
  stanSamples %>%
    GGally::ggpairs(
      lower =
        list(
          continuous = 
            function(data, mapping, ...) {
              xMapping <-
                mapping$x %>%
                rlang::f_rhs() %>%
                as.character()
              
              yMapping <-
                mapping$y %>%
                rlang::f_rhs() %>%
                as.character()
              
              stanXMean = stanEstimatesVector[xMapping]
              stanYMean = stanEstimatesVector[yMapping]
              
              ggplot(data = data, mapping = mapping) + 
                geom_density_2d_filled(contour_var = 'density') +
                geom_hline(
                  yintercept = stanYMean,
                  alpha = 0.75,
                  colour = 'red',
                  linetype = 'dashed'
                ) +
                geom_vline(
                  xintercept = stanXMean,
                  alpha = 0.75,
                  colour = 'red',
                  linetype = 'dashed'
                )
            }
        ),
      diag =
        list(
          continuous =
            function(data, mapping, ...) {
              xMapping <- mapping$x %>% rlang::f_rhs() %>% as.character()
              
              stanXMean = stanEstimatesVector[xMapping]
              
              GGally::ggally_densityDiag(data, mapping) +
                geom_vline(
                  xintercept = stanXMean,
                  alpha = 0.75,
                  colour = 'red',
                  linetype = 'dashed'
                )
            }
        )
    ) +
    ggtitle(
      label = 'Pairwise plot of parameter estimates.'
    ) +
    cowplot::theme_half_open() +
    cowplot::background_grid() +
    theme(
      axis.text = element_text(size = 8.0),
      strip.text = element_text(size = 8.0),
      panel.border = element_rect(colour = 'black')
    )
})
```


### Posteriors vs Priors

Extending this further, it would be of interest to see how the marginal distributions along the diagonal compared against our original priors and the point-estimates as provided by the GLM approach.

* The <span style="color: blue;">blue</span> lines show the densities of the **prior** distributions.
* Where we have a stronger belief in our current basis, we have indicated this by specifying "tighter" prior variances, such as for $\mu_{BASE}^{ADJ}$ and $O_{LOW}^{ADJ}$; hence, we see leaner tails.
* For the remaining parameters where we are less convinced, we have specified relatively larger variances as indicated by the fatter tails of those distributions.
* For those parameters with stronger priors, we _might_ expect the resulting posterior distribution (<span style="color: red;">red</span> lines), and corresponding posterior point-estimate to be more weighted towards our prior belief than our GLM point-estimate; the latter of which has no such prior weighting.
* However, any such weighting will also be influenced by the prior strength of any other parameters with which they are correlated.
* As such, one of the main benefits of this approach is that the posterior-estimates of all parameters consider not only their own priors and actual observations, but those of all other parameters with which they are (in)directly linked.

```{r showDistributionComparisons, include = TRUE}
local({
  distnSummary <-
    stanSamples %>%
    tidyr::pivot_longer(
      cols = dplyr::everything(),
      names_to = 'LABEL_CHAR',
      values_to = 'SAMPLE'
    ) %>%
    dplyr::group_by(
      LABEL_CHAR
    ) %>%
    dplyr::summarise(
      POSTERIOR_DENSITY_FN =
        list(
          approxfun(
            density(SAMPLE),
            yleft = 0.0,
            yright = 0.0
          )
        ),
      POSTERIOR_DENSITY_MIN =
        min(SAMPLE),
      POSTERIOR_DENSITY_MAX =
        max(SAMPLE),
      .groups = 'drop'
    ) %>%
    dplyr::left_join(
      priorParams %>%
        purrr::map_dfr(
          .id = 'LABEL_CHAR',
          tibble::enframe,
          name = 'PARAMETER',
          value = 'VALUE'
        ) %>%
        tidyr::pivot_wider(
          names_from = PARAMETER,
          names_prefix = 'PRIOR_',
          values_from = VALUE
        ),
      by = 'LABEL_CHAR'
    ) %>%
    dplyr::left_join(
      stanEstimates %>%
        dplyr::transmute(
          LABEL,
          LABEL_CHAR = as.character(LABEL),
          POSTERIOR_ESTIMATE = FITTED_ADJUSTMENT
        ),
      by = 'LABEL_CHAR'
    ) %>%
    dplyr::mutate(
      PRIOR_DENSITY_FN =
        purrr::map2(
          PRIOR_MEAN,
          PRIOR_SD,
          function (mean, sd) {
            function (x)
              dnorm(log(x), mean = mean, sd = sd)
          }
        ),
      PRIOR_DENSITY_MIN =
        exp(PRIOR_MEAN - 3.0 * PRIOR_SD),
      PRIOR_DENSITY_MAX =
        exp(PRIOR_MEAN + 3.0 * PRIOR_SD),
    ) %>%
    dplyr::left_join(
      glmEstimates %>%
        dplyr::transmute(
          LABEL,
          GLM_ESTIMATE =
            FITTED_ADJUSTMENT
        ),
      by = 'LABEL'
    ) %>%
    dplyr::mutate(
      GLM_MIN =
        GLM_ESTIMATE - PRIOR_SD,
      GLM_MAX =
        GLM_ESTIMATE + PRIOR_SD
    ) %>%
    dplyr::rowwise() %>%
    dplyr::mutate(
      OVERALL_MIN =
        min(
          dplyr::across(
            dplyr::ends_with('MIN')
          )
        ) %>%
        max(0.0),
      OVERALL_MAX =
        max(
          dplyr::across(
            dplyr::ends_with('MAX')
          )
        ),
      PRIOR_DENSITY_PLOT =
        list(
          geom_function(
            data = dplyr::pick(LABEL),
            aes(colour = 'PRIOR'),
            fun = PRIOR_DENSITY_FN,
            xlim = c(OVERALL_MIN, OVERALL_MAX)
          )
        ),
      POSTERIOR_DENSITY_PLOT =
        list(
          geom_function(
            data = dplyr::pick(LABEL),
            aes(colour = 'POSTERIOR'),
            fun = POSTERIOR_DENSITY_FN,
            xlim = c(OVERALL_MIN, OVERALL_MAX)
          )
        )
    ) %>%
    dplyr::ungroup() %>%
    dplyr::select(
      LABEL,
      PRIOR_DENSITY_PLOT,
      POSTERIOR_DENSITY_PLOT,
      POSTERIOR_ESTIMATE,
      GLM_ESTIMATE
    )

  with(
    distnSummary,
    c(
      PRIOR_DENSITY_PLOT,
      POSTERIOR_DENSITY_PLOT
    )
  ) %>%
  purrr::reduce(
    .f = magrittr::add,
    .init = ggplot()
  ) +
  scale_colour_manual(
    name = 'Density:',
    labels = c(POSTERIOR = 'Posterior', PRIOR = 'Prior'),
    values = c(POSTERIOR = 'red', PRIOR = 'blue')
  ) +
  ggnewscale::new_scale_colour() +
  geom_vline(
    data = distnSummary,
    aes(
      xintercept = GLM_ESTIMATE,
      colour = 'GLM',
    ),
    linetype = 'dashed'
  ) +
  geom_vline(
    data = distnSummary,
    aes(
      xintercept = POSTERIOR_ESTIMATE,
      colour = 'POSTERIOR_ESTIMATE'
    ),
    linetype = 'dashed'
  ) +
  facet_wrap(
    LABEL ~ .,
    scales = 'free'
  ) +
  scale_colour_manual(
    name = 'Estimates:',
    labels = c(GLM = 'GLM', POSTERIOR_ESTIMATE = 'Bayesian'),
    values = c(GLM = 'black', POSTERIOR_ESTIMATE = 'red')
  ) +
  scale_x_continuous(
    name = 'Parameter Value'
  ) +
  scale_y_continuous(
    name = 'Density'
  ) +
  ggtitle(
    label = 'Comparison of priors, posteriors and estimates.'
  ) +
  cowplot::theme_half_open() +
  cowplot::background_grid() +
  theme(
    legend.title = element_text(face = 'bold'),
    legend.position = 'bottom'
  )
})
```

```{r augmentPolicyDataBayesian}
augmentedPolDataBayesian <-
  purrr::reduce(
    c('BASE', 'GENDER', 'HEALTH', 'OCC_RISK'),
    function (prior, dimension) {
      prior %>%
        # Without this, we seem to lose the ordered factors within
        # the respective dimensional columns.
        dplyr::mutate(
          LEVEL = .data[[dimension]]
        ) %>%
        dplyr::left_join(
          stanEstimates %>%
            dplyr::filter(
              DIMENSION == dimension
            ) %>%
            dplyr::transmute(
              LEVEL,
              '{dimension}_LOADING' := BAYESIAN_PROPOSED
            ),
          by = 'LEVEL'
        ) %>%
        dplyr::select(
          -LEVEL
        )
    },
    .init =
      augmentedPolDataGLM %>%
      dplyr::mutate(
        BASE = 'BASE'
      )
  ) %>%
  dplyr::rowwise() %>%
  dplyr::mutate(
    BAYESIAN_RATE =
      prod(
        dplyr::c_across(
          dplyr::ends_with('LOADING')
        )
      )
  ) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(
    GENDER_ALT,
    HEALTH_ALT,
    OCC_RISK_ALT
  ) %>%
  dplyr::transmute(
    GENDER,
    HEALTH,
    OCC_RISK,
    GENDER_ALT,
    HEALTH_ALT,
    OCC_RISK_ALT,
    PROPORTION,
    EXPOSURE,
    ACTUAL_RATE,
    EXPECTED_RATE,
    GLM_RATE,
    BAYESIAN_RATE,
    ACTUAL_DEATHS,
    EXPECTED_DEATHS,
    GLM_DEATHS =
      EXPOSURE * GLM_RATE,
    BAYESIAN_DEATHS =
      EXPOSURE * BAYESIAN_RATE
  )
```

### Proposed Fit {.tabset .tabset-fade .tabset-pills}

```{r prepareBayesianFitTable}
bayesianFitTable <-
  local({
    preTotals <-
      augmentedPolDataBayesian %>%
      dplyr::transmute(
        GENDER,
        HEALTH,
        OCC_RISK,
        EXPOSURE,
        ACTUAL_DEATHS,
        EXPECTED_DEATHS,
        GLM_DEATHS,
        BAYESIAN_DEATHS,
        ACTUAL_RATE,
        EXPECTED_RATE,
        GLM_RATE,
        BAYESIAN_RATE
      )
    
    totalRow <-
      augmentedPolDataBayesian %>%
      dplyr::summarise(
        OCC_RISK = 'TOTAL',
        EXPECTED_RATE = sum(EXPOSURE * EXPECTED_RATE) / sum(EXPOSURE),
        GLM_RATE = sum(EXPOSURE * GLM_RATE) / sum(EXPOSURE),
        BAYESIAN_RATE = sum(EXPOSURE * BAYESIAN_RATE) / sum(EXPOSURE),
        EXPOSURE = sum(EXPOSURE),
        ACTUAL_DEATHS = sum(ACTUAL_DEATHS),
        EXPECTED_DEATHS = sum(EXPECTED_DEATHS),
        GLM_DEATHS = sum(GLM_DEATHS),
        BAYESIAN_DEATHS = sum(BAYESIAN_DEATHS),
        ACTUAL_RATE = ACTUAL_DEATHS / EXPOSURE
      )
    
    preTotals %>%
      dplyr::bind_rows(totalRow) %>%
      dplyr::transmute(
        GENDER =
          GENDER %>%
          as.character() %>%
          tidyr::replace_na(replace = ''),
        HEALTH =
          HEALTH %>%
          as.character() %>%
          tidyr::replace_na(replace = ''),
        OCC_RISK,
        EXPOSURE =
          scales::comma(EXPOSURE),
        EXPECTED_DEATHS =
          scales::comma(EXPECTED_DEATHS, accuracy = 1.0),
        ACTUAL_DEATHS =
          scales::comma(ACTUAL_DEATHS, accuracy = 1.0),
        GLM_DEATHS =
          scales::comma(GLM_DEATHS, accuracy = 1.0),
        BAYESIAN_DEATHS =
          scales::comma(BAYESIAN_DEATHS, accuracy = 1.0),
        EXPECTED_RATE =
          scales::percent(EXPECTED_RATE, accuracy = 0.1),
        ACTUAL_RATE =
          scales::percent(ACTUAL_RATE, accuracy = 0.1),
        GLM_RATE =
          scales::percent(GLM_RATE, accuracy = 0.1),
        BAYESIAN_RATE =
          scales::percent(BAYESIAN_RATE, accuracy = 0.1)
      )
  })

bayesianFitFormat <-
  function (data) {
    data %>%
      knitr::kable(
        format = 'html',
        align = 'cccccccc'
      ) %>%
      kableExtra::kable_styling(
        bootstrap_options = 'bordered',
        full_width = FALSE
      ) %>%
      kableExtra::column_spec(
        column = 1L:3L,
        background = '#EEEEEE'
      ) %>%
      kableExtra::row_spec(
        row = 0L,
        bold = TRUE,
        background = '#BBBBBB'
      ) %>%
      kableExtra::row_spec(
        row = nrow(data),
        bold = TRUE,
        background = '#EEEEEE'
      )
  }
```

#### Deaths

```{r showBayesianDeaths, include = TRUE}
bayesianFitTable %>%
  dplyr::select(
    -dplyr::ends_with('RATE')
  ) %>%
  dplyr::rename_with(
    stringr::str_remove,
    pattern = '_DEATHS$'
  ) %>%
  bayesianFitFormat()
```

#### Rates

```{r showBayesianRates, include = TRUE}
bayesianFitTable %>%
  dplyr::select(
    -dplyr::ends_with('DEATHS')
  ) %>%
  dplyr::rename_with(
    stringr::str_remove,
    pattern = '_RATE$'
  ) %>%
  bayesianFitFormat()
```

### Summary

By using a Bayesian approach when fitting to our actual experience, we are better able to allow for any prior beliefs we might have for our current basis. Furthermore, this is considered holistically as opposed to the current mechanical-rules approach.

However, there are some additional considerations:

* Additional complexity that needs to be explained/justified.
* Reasoning for direction and extent of posterior estimates not always obvious.
* Distributions have to be derived for our prior estimates.